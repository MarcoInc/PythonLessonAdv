{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOTO MINIMO -> map(LAMBDA).min() SENZA REDUCE\n",
      "5\n",
      "Ordinamento in base alla Chiave -> map(LAMBDA) e sortBy\n",
      "[('Bianchi A.', '30'), ('Rossi M.', '25'), ('Verdi C.', '18'), ('Gialli E.', '10'), ('Marroni D.', '05')]\n"
     ]
    }
   ],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "#SparkContext è contenuto nella classe SparkSession\n",
    "\n",
    "#MAP -> è una funzione di trasformazione\n",
    "    #argomento -> un RDD\n",
    "    #risultato -> un RDD -> passato al REDUCE o altra funzione\n",
    "#REDUCE -> azione di sintesi -> può essere sostituita da una funzione\n",
    "\n",
    "candidati=['Rossi M. 25','Bianchi A. 30','Verdi C. 18',\n",
    "            'Marroni D. 05','Gialli E. 10',]\n",
    "#Consideriamo che il voto è in 30esimi \n",
    "# e che corrisponde agli ultimi 2 caratteri\n",
    "\n",
    "#CREIAMO LO SparkContex\n",
    "    #getOrCreate() -> impedisce Context MULTIPLI\n",
    "sc=SparkContext.getOrCreate()\n",
    "\n",
    "# sc=SparkContext('local','esempio')\n",
    "\n",
    "\n",
    "#CREIAMO L'RDD\n",
    "rdd_candidati=sc.parallelize(candidati)\n",
    "\n",
    "print(\"VOTO MINIMO -> map(LAMBDA).min() SENZA REDUCE\")\n",
    "#Troviamo il minimo \n",
    "    # min() usato al posto di una reduce\n",
    "voto_min=rdd_candidati.map(lambda voto: int (voto[-2:])).min()\n",
    "#Fa una map di tutti gli ultimi 2 caratteri per e dopo calcola il min()\n",
    "    #la lambda ritorna interi\n",
    "print(voto_min)\n",
    "\n",
    "print(\"Ordinamento in base alla Chiave -> map(LAMBDA) e sortBy\")\n",
    "\n",
    "#Scorre l'RDD eeparia ogni elemento dell'RRD con una virgola\",\" \n",
    "    # contiene una sorta di Lista di liste\n",
    "token=rdd_candidati.map(lambda x: x.split())\n",
    "# [['Rossi', 'M.', '25'], \n",
    "#  ['Bianchi', 'A.', '30'], \n",
    "#  ['Verdi', 'C.', '18'], \n",
    "#  ['Marroni', 'D.', '05'], \n",
    "#  ['Gialli', 'E.', '10']]\n",
    "\n",
    "#Scorre token e Separa Nome[0] e Cognome[1] dal Voto[2] \n",
    "    # ad ogni iterazione ritorna una tupla chiave/valore\n",
    "                                        #f'({CHI}{AVE},{VALORE})'\n",
    "key_value=token.map(lambda parti: (f'{parti[0]} {parti[1]}',parti[2]))\n",
    "#nuovo risultato in key_value -> (Nome Cognome)[0] Voto[1].\n",
    "#[('Rossi M.', '25'),                    #CHIAVE  / VALORE\n",
    "# ('Bianchi A.', '30'), \n",
    "# ('Verdi C.', '18'), \n",
    "# ('Marroni D.', '05'), \n",
    "# ('Gialli E.', '10')]\n",
    "\n",
    "#ordina tutto in modo decrescente in base al valore -> voto[1] di key_value\n",
    "sorted_key_value=key_value.sortBy(lambda x: x[1], ascending=False)\n",
    "#mette i dati ordinati in sorted_key\n",
    "\n",
    "#CONVERTIAMO L'RDD degli ordinati IN UNA LISTA ris\n",
    "ris=sorted_key_value.collect()\n",
    "print(ris)\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOTO MINIMO -> map(LAMBDA).min() SENZA REDUCE\n",
      "[('PARI', 2), ('DISPARI', 9), ('PARI', 16), ('DISPARI', 57), ('DISPARI', 95), ('DISPARI', 3), ('DISPARI', 1), ('DISPARI', 51), ('PARI', 16), ('PARI', 856), ('DISPARI', 1), ('PARI', 8), ('DISPARI', 633), ('DISPARI', 9), ('DISPARI', 7), ('DISPARI', 1), ('DISPARI', 45), ('PARI', 6)]\n",
      "[('DISPARI', 912), ('PARI', 904)]\n",
      "[('DISPARI', 912), ('PARI', 904)]\n"
     ]
    }
   ],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "\n",
    "numeri=[2,9,16,57,95,3,1,51,16,856,1,8,633,9,7,1,45,6]\n",
    "\n",
    "sc=SparkContext.getOrCreate()\n",
    "\n",
    "# sc=SparkContext('local','esempio')\n",
    "\n",
    "\n",
    "#CREIAMO L'RDD\n",
    "rdd_numeri=sc.parallelize(numeri)\n",
    "\n",
    "print(\"VOTO MINIMO -> map(LAMBDA).min() SENZA REDUCE\")\n",
    "\n",
    "#Crea coppie->tuple chiave/valore stringa/numero\n",
    "# in cui associamo DISPARI se il numero è dispari altrimenti PARI se è pari\n",
    "pari_dispari=rdd_numeri.map(lambda n:('DISPARI' if n%2 else 'PARI',n))\n",
    "#[('PARI', 2), ('DISPARI', 9),  ('PARI', 16), \n",
    "# ('DISPARI', 57), ('DISPARI', 95), ('DISPARI', 3), \n",
    "# ('DISPARI', 1), ('DISPARI', 51), ('PARI', 16), \n",
    "# ('PARI', 856), ('DISPARI', 1), ('PARI', 8), \n",
    "# ('DISPARI', 633), ('DISPARI', 9), ('DISPARI', 7), \n",
    "# ('DISPARI', 1), ('DISPARI', 45), ('PARI', 6)]\n",
    "\n",
    "\n",
    "#Raccoglie le coppie per chiave a cui il valore associa a e b\n",
    "    # -> a <- somma parziale\n",
    "    # -> b <- valore chie viene sommato via via ad a\n",
    "reduced_pari_dispari=pari_dispari.reduceByKey(lambda a,b: a+b)\n",
    "#[('DISPARI', 912), ('PARI', 904)]\n",
    "\n",
    "ris=reduced_pari_dispari.collect()\n",
    "print(ris)\n",
    "\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
