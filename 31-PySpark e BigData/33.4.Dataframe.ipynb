{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leggo un file csv-> spark.read.csv\n",
      "Tipi di dato intuiti da PySpark-> df.dtypes\n",
      "[('nome', 'string'), ('cognome', 'string'), ('altezza', 'double'), ('peso', 'int'), ('nascita', 'date'), ('sesso', 'string')]\n"
     ]
    }
   ],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "#Per usare i dataframe si usa SparkSession contenuto in pyspark.sql\n",
    "from pyspark.sql import SparkSession #SparkContext è contentuto in SparkSession\n",
    "from pyspark import SparkContext #usato successivamente per creare il RDD\n",
    "\n",
    "#CREO UN OGGETTO SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"Leggo un file csv-> spark.read.csv\")\n",
    "#Lettura contenuto file csv\n",
    "dati_csv=spark.read.csv('33.4.persone.csv', sep=';', inferSchema=True,header=True)\n",
    "# nome del file\n",
    "# sep= -> separatore\n",
    "# inferSchema=True -> permette a PySpark di intuire il tipo di dato\n",
    "# header=True -> la prima riga contiene i nomi delle colonne\n",
    "\n",
    "print(\"Tipi di dato intuiti da PySpark-> df.dtypes\")\n",
    "print(dati_csv.dtypes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCESSO AL DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mario', 'Rossi', 1.7, 43, '1999-12-25', 'M'), ('Carla', 'Verdi', 1.5, 54, '1990-08-09', 'F'), ('Marta', 'Bianchi', 1.75, 82, '2000-11-28', 'F'), ('Pippo', 'Neri', 1.65, 77, '2001-10-07', 'M')]\n",
      "Dataframe vista con -> show()\n",
      "+-----+-------+-------+----+----------+-----+\n",
      "| nome|cognome|altezza|peso|   nascita|sesso|\n",
      "+-----+-------+-------+----+----------+-----+\n",
      "|Mario|  Rossi|    1.7|  43|1999-12-25|    M|\n",
      "|Carla|  Verdi|    1.5|  54|1990-08-09|    F|\n",
      "|Marta|Bianchi|   1.75|  82|2000-11-28|    F|\n",
      "|Pippo|   Neri|   1.65|  77|2001-10-07|    M|\n",
      "+-----+-------+-------+----+----------+-----+\n",
      "\n",
      "Prime n righe DataFrame -> show(n)\n",
      "+-----+-------+-------+----+----------+-----+\n",
      "| nome|cognome|altezza|peso|   nascita|sesso|\n",
      "+-----+-------+-------+----+----------+-----+\n",
      "|Mario|  Rossi|    1.7|  43|1999-12-25|    M|\n",
      "+-----+-------+-------+----+----------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "Count :  4\n",
      "First :  Row(nome='Mario', cognome='Rossi', altezza=1.7, peso=43, nascita='1999-12-25', sesso='M')\n",
      "Take il 2:  [Row(nome='Mario', cognome='Rossi', altezza=1.7, peso=43, nascita='1999-12-25', sesso='M'), Row(nome='Carla', cognome='Verdi', altezza=1.5, peso=54, nascita='1990-08-09', sesso='F')]\n",
      "Head primi 2 :  [Row(nome='Mario', cognome='Rossi', altezza=1.7, peso=43, nascita='1999-12-25', sesso='M'), Row(nome='Carla', cognome='Verdi', altezza=1.5, peso=54, nascita='1990-08-09', sesso='F')]\n",
      "Describe :  DataFrame[summary: string, nome: string, cognome: string, altezza: string, peso: string, nascita: string, sesso: string]\n",
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- cognome: string (nullable = true)\n",
      " |-- altezza: double (nullable = true)\n",
      " |-- peso: long (nullable = true)\n",
      " |-- nascita: string (nullable = true)\n",
      " |-- sesso: string (nullable = true)\n",
      "\n",
      "printSchema :  None\n",
      "ESTRAGGO IL CAMPO DEL PRIMO DATO -> first().campo\n",
      "Primo nome :  Mario\n",
      "Primo cognome :  Rossi\n",
      "Primo peso :  43\n",
      "Peso con take(2)[1].peso:  54\n",
      "Cognome con take(2)[0].cognome:  Rossi\n"
     ]
    }
   ],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "#Per usare i dataframe si usa SparkSession contenuto in pyspark.sql\n",
    "from pyspark.sql import SparkSession #SparkContext è contentuto in SparkSession\n",
    "from pyspark import SparkContext #usato successivamente per creare il RDD\n",
    "\n",
    "\n",
    "#TRA RDD E DataFrame ci sono due generazioni diverse\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "dati=[('Mario','Rossi',1.7,43,'1999-12-25','M'),\n",
    "('Carla','Verdi',1.5,54,'1990-08-09','F'),\n",
    "('Marta','Bianchi',1.75,82,'2000-11-28','F'),\n",
    "('Pippo','Neri',1.65,77,'2001-10-07','M')]\n",
    "\n",
    "#creo uno sparkContext\n",
    "sc=spark.sparkContext\n",
    "\n",
    "rdd=sc.parallelize(dati)\n",
    "print(rdd.collect())\n",
    "\n",
    "#Creare un DataFrame da un RDD specificando:\n",
    "    #RDD di partenza\n",
    "    #Nome colonne\n",
    "dati_da_rdd=spark.createDataFrame(rdd,['nome','cognome',\n",
    "                                    'altezza','peso','nascita','sesso'])\n",
    "print('Dataframe vista con -> show()')\n",
    "dati_da_rdd.show()\n",
    "print('Prime n righe DataFrame -> show(n)')\n",
    "dati_da_rdd.show(1)\n",
    "\n",
    "#count() -> conta le righe\n",
    "print(\"Count : \",dati_da_rdd.count())\n",
    "\n",
    "#first() -> ritorna il primo record\n",
    "print(\"First : \",dati_da_rdd.first())\n",
    "\n",
    "#take() -> prende un certo numero di righe\n",
    "print(\"Take il 2: \",dati_da_rdd.take(2)) # OGGETTO ROW\n",
    "\n",
    "#head() -> prende un certo numero di righe\n",
    "print(\"Head primi 2 : \",dati_da_rdd.head(2))  # OGGETTO ROW\n",
    "\n",
    "#describe() -> ritorna vari dati come conteggio, media, dev standard, max e min\n",
    "print(\"Describe : \",dati_da_rdd.describe())\n",
    "\n",
    "#printSchema() -> stampa la struttura del Dataframe\n",
    "print(\"printSchema : \",dati_da_rdd.printSchema())\n",
    "\n",
    "\n",
    "print('ESTRAGGO IL CAMPO DEL PRIMO DATO -> first().campo')\n",
    "#Dot Syntax\n",
    "#I tipi estratti saranno quelli predefiniti di Python\n",
    "print(\"Primo nome : \",dati_da_rdd.first().nome) #TIPO STRINGA\n",
    "print(\"Primo cognome : \",dati_da_rdd.first().cognome) #TIPO STRINGA\n",
    "print(\"Primo peso : \",dati_da_rdd.first().peso) #TIPO FLOAT\n",
    "\n",
    "#Prende le prime 2\n",
    "    #Estraggo una singola ROW\n",
    "    \n",
    "#Prendo la 1 estratta\n",
    "    #prendo il solo peso\n",
    "print(\"Peso con take(2)[1].peso: \",dati_da_rdd.take(2)[1].peso) #TIPO FLOAT\n",
    "#Prendo la 0 estratta\n",
    "    #prendo il solo cognome\n",
    "print(\"Cognome con take(2)[0].cognome: \",dati_da_rdd.take(2)[0].cognome) #TIPO STRINGA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
